EX - 2
    !pip install -q tensorflow pandas scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, Callback

# STEP 1: Load dataset
df = pd.read_csv("/content/drive/MyDrive/DT Team -1/Student_performance_data _.csv")
df.columns = df.columns.str.strip()  # Clean column names

# STEP 2: Drop unnecessary columns
df = df.drop("StudentID", axis=1)

# STEP 3: Split features and target
X = df.drop("GradeClass", axis=1)
y = df["GradeClass"].astype(int)

# STEP 4: Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# STEP 5: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# STEP 6: Define model parameters
activation_1 = 'relu'
activation_2 = 'relu'
activation_3 = 'softmax'  # For multi-class classification
learning_rate = 0.001
batch_size = 32
loss_function = 'sparse_categorical_crossentropy'
epochs = 50

# STEP 7: Build the model
model = Sequential([
    Dense(128, activation=activation_1, input_shape=(X.shape[1],)),
    Dropout(0.3),
    Dense(64, activation=activation_2),
    Dropout(0.2),
    Dense(len(y.unique()), activation=activation_3)  # Output = number of classes
])

# STEP 8: Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])

# STEP 9: Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# STEP 10: Custom callback to record test accuracy
class TestAccuracyCallback(Callback):
    def __init__(self, test_data):
        self.test_data = test_data
        self.test_accuracies = []

    def on_epoch_end(self, epoch, logs=None):
        test_loss, test_acc = self.model.evaluate(self.test_data[0], self.test_data[1], verbose=0)
        self.test_accuracies.append(test_acc)

test_callback = TestAccuracyCallback((X_test, y_test))

# STEP 11: Train the model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop, test_callback],
    verbose=1
)

# STEP 12: Summary table
train_accuracies = history.history['accuracy']
val_accuracies = history.history['val_accuracy']
losses = history.history['loss']
val_losses = history.history['val_loss']
test_accuracies = test_callback.test_accuracies

summary_df = pd.DataFrame({
    'Epoch': list(range(1, len(train_accuracies) + 1)),
    'Learning Rate': [learning_rate] * len(train_accuracies),
    'Activation Functions': [f'{activation_1}, {activation_2}, {activation_3}'] * len(train_accuracies),
    'Batch Size': [batch_size] * len(train_accuracies),
    'Loss Function': [loss_function] * len(train_accuracies),
    'Train Accuracy': [round(v, 4) for v in train_accuracies],
    'Val Accuracy': [round(v, 4) for v in val_accuracies],
    'Test Accuracy': [round(v, 4) for v in test_accuracies],
    'Train Loss': [round(v, 4) for v in losses],
    'Val Loss': [round(v, 4) for v in val_losses],
})

print("\nðŸ“Š Training Summary Table:")
print(summary_df.to_string(index=False))

# STEP 13: Final evaluation
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\nâœ… Final Test Accuracy: {acc * 100:.2f}%")

# STEP 14: Classification report
y_pred = np.argmax(model.predict(X_test), axis=-1)
print("\nðŸ“„ Classification Report:")
print(classification_report(y_test, y_pred))

# STEP 15: Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

df.head()

